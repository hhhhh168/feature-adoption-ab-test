{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis - Verification Flow Experiment\n",
    "\n",
    "Quick exploration of the data before formal analysis. Just checking things look right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# load data\n",
    "assignments = pd.read_csv('../data/experiment_assignments.csv')\n",
    "verification = pd.read_csv('../data/verification_attempts.csv')\n",
    "users = pd.read_csv('../data/users.csv')\n",
    "\n",
    "print(f\"Users: {len(users)}\")\n",
    "print(f\"Assignments: {len(assignments)}\")\n",
    "print(f\"Verification attempts: {len(verification)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check on variant split\n",
    "assignments['variant'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks close to 50/50, thats good\n",
    "# merge to get conversion\n",
    "\n",
    "tier1 = verification[verification['verification_tier'] == 1].copy()\n",
    "tier1['converted'] = (tier1['completion_status'] == 'completed').astype(int)\n",
    "\n",
    "df = assignments.merge(tier1[['user_id', 'converted']], on='user_id', how='left')\n",
    "df['converted'] = df['converted'].fillna(0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion by variant\n",
    "df.groupby('variant')['converted'].agg(['sum', 'count', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hmm treatment looks a bit higher but sample is small\n",
    "# lets check by device\n",
    "\n",
    "df.groupby(['variant', 'device_type'])['converted'].mean().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesting, iOS slightly higher in control\n",
    "# treatment effect seems similar across devices though\n",
    "\n",
    "# quick viz\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# conversion by variant\n",
    "conv_by_variant = df.groupby('variant')['converted'].mean()\n",
    "axes[0].bar(conv_by_variant.index, conv_by_variant.values)\n",
    "axes[0].set_title('Conversion by Variant')\n",
    "axes[0].set_ylabel('Conversion Rate')\n",
    "\n",
    "# by device\n",
    "conv_pivot = df.groupby(['variant', 'device_type'])['converted'].mean().unstack()\n",
    "conv_pivot.plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_title('Conversion by Variant & Device')\n",
    "axes[1].set_ylabel('Conversion Rate')\n",
    "axes[1].legend(title='Device')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pre-experiment metrics are balanced\n",
    "pre_metrics = pd.read_csv('../data/user_pre_metrics.csv')\n",
    "df_with_pre = df.merge(pre_metrics, on='user_id')\n",
    "\n",
    "# t-tests for balance\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "for col in ['pre_sessions_count', 'pre_matches_count', 'pre_messages_sent']:\n",
    "    ctrl = df_with_pre[df_with_pre['variant'] == 'control'][col]\n",
    "    trt = df_with_pre[df_with_pre['variant'] == 'treatment'][col]\n",
    "    t, p = ttest_ind(ctrl, trt)\n",
    "    print(f\"{col}: t={t:.3f}, p={p:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all balanced, good\n",
    "# correlation between pre and post for CUPED?\n",
    "\n",
    "# need to aggregate post metrics... actually lets just check sessions\n",
    "events = pd.read_csv('../data/events.csv')\n",
    "post_sessions = events.groupby('user_id').size().reset_index(name='post_sessions')\n",
    "\n",
    "df_cuped = df_with_pre.merge(post_sessions, on='user_id', how='left')\n",
    "df_cuped['post_sessions'] = df_cuped['post_sessions'].fillna(0)\n",
    "\n",
    "# correlation\n",
    "corr = df_cuped[['pre_sessions_count', 'post_sessions']].corr().iloc[0,1]\n",
    "print(f\"Pre-post sessions correlation: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decent correlation, CUPED should help\n",
    "# variance reduction estimate: r^2 = correlation^2\n",
    "print(f\"Expected variance reduction: {corr**2:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- Sample split looks fine (~50/50)\n",
    "- Treatment shows ~15% lift but need formal test\n",
    "- Pre-metrics balanced\n",
    "- CUPED should give ~40% variance reduction\n",
    "- Sample might be too small for significance... need to check power"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
