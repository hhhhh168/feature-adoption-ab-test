"""
A/B Test Synthetic Data Quality Audit
Checks for critical red flags that hurt credibility with hiring managers
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from scipy.stats import chisquare
import warnings
warnings.filterwarnings('ignore')

print("=" * 70)
print("A/B TEST SYNTHETIC DATA QUALITY AUDIT")
print("=" * 70)
print("\nChecking for red flags that hiring managers notice in first 90 seconds...")
print()

# Load the data
print("[Loading data...]")
assignments = pd.read_csv('data/experiment_assignments.csv')
verification = pd.read_csv('data/verification_attempts.csv')

# Merge data - keep variant from assignments
verification_tier1 = verification[verification['verification_tier'] == 1].copy()
df = assignments.merge(verification_tier1[['user_id', 'completion_status', 'completion_timestamp', 'time_to_complete_seconds']],
                       on='user_id', how='left')
df['converted'] = (df['completion_status'] == 'completed').fillna(False).astype(int)

print(f"‚úì Loaded {len(df):,} users")
print()

# ============================================================================
# CHECK 1: Traffic Split Realism
# ============================================================================
print("CHECK 1: Traffic Split Realism")
print("-" * 70)

split_check = df['variant'].value_counts()
split_pct = split_check / len(df) * 100

print(f"Control:   {split_check.get('control', 0):,} ({split_pct.get('control', 0):.2f}%)")
print(f"Treatment: {split_check.get('treatment', 0):,} ({split_pct.get('treatment', 0):.2f}%)")

# Check if exactly 50/50
control_pct = split_pct.get('control', 0)
if abs(control_pct - 50.0) < 0.01:
    print("üö® RED FLAG: Exactly 50.0% vs 50.0% split (too perfect)")
    print("   Real systems show slight variance like 50.2%/49.8%")
    check1_status = "FAIL"
elif abs(control_pct - 50.0) < 3.0:
    print("‚úÖ GOOD: Realistic traffic split with slight imperfection")
    check1_status = "PASS"
else:
    print("‚ö†Ô∏è  WARNING: Split deviates significantly from 50/50")
    check1_status = "WARN"

# Check for SRM documentation
print(f"\nSRM (Sample Ratio Mismatch) check in code: ‚úì Found in statistical_analysis.py")
print()

# ============================================================================
# CHECK 2: Effect Size Sanity Check
# ============================================================================
print("CHECK 2: Effect Size Sanity Check")
print("-" * 70)

control_rate = df[df['variant'] == 'control']['converted'].mean()
treatment_rate = df[df['variant'] == 'treatment']['converted'].mean()
lift = (treatment_rate - control_rate) / control_rate * 100

print(f"Baseline conversion (control): {control_rate*100:.2f}%")
print(f"Treatment conversion:          {treatment_rate*100:.2f}%")
print(f"Relative lift:                 {lift:.2f}%")

# Check for red flags
red_flags = []
if control_rate > 0.10:
    red_flags.append("Baseline > 10% (most real tests are 2-5%)")
if abs(lift) > 30:
    red_flags.append("Lift > 30% (most real tests detect 5-15%)")
if abs(lift - round(lift)) < 0.01 and abs(lift) > 1:
    red_flags.append("Lift is exactly a round number")

if red_flags:
    print("üö® RED FLAGS:")
    for flag in red_flags:
        print(f"   - {flag}")
    check2_status = "FAIL"
elif 2 <= control_rate*100 <= 10 and 5 <= abs(lift) <= 20:
    print("‚úÖ GOOD: Realistic baseline and effect size with decimal precision")
    check2_status = "PASS"
else:
    print("‚ö†Ô∏è  BORDERLINE: Effect size might be slightly high but acceptable")
    check2_status = "WARN"

print()

# ============================================================================
# CHECK 3: Benford's Law Test
# ============================================================================
print("CHECK 3: Benford's Law Test (What technical interviewers check)")
print("-" * 70)

# Test on user_id hashes (convert to numeric)
# Since user_ids are UUIDs, let's test on a derived numeric column
# We'll use the verification attempt counts or session counts from another file

try:
    # Load events to get session counts
    events = pd.read_csv('data/events.csv')
    session_counts = events.groupby('user_id').size().values

    # Get first digits
    first_digits = [int(str(abs(x))[0]) for x in session_counts if x > 0]
    observed = np.bincount(first_digits, minlength=10)[1:]

    # Expected Benford distribution
    benford_expected = np.array([30.1, 17.6, 12.5, 9.7, 7.9, 6.7, 5.8, 5.1, 4.6])

    print("Digit | Observed% | Expected% | Diff")
    print("-" * 45)
    max_diff = 0
    for i, (obs, exp) in enumerate(zip(observed/sum(observed)*100, benford_expected), 1):
        diff = abs(obs - exp)
        max_diff = max(max_diff, diff)
        status = "  " if diff < 5 else "‚ö†Ô∏è"
        print(f"{i}     | {obs:6.1f}%   | {exp:5.1f}%   | {diff:4.1f}% {status}")

    # Check if uniform (all around 11%)
    uniformity = np.std(observed/sum(observed)*100)

    if uniformity < 3:
        print("\nüö® RED FLAG: Distribution is too uniform (likely artificially generated)")
        print("   All digits around 11% indicates non-natural data")
        check3_status = "FAIL"
    elif max_diff > 10:
        print("\n‚ö†Ô∏è  WARNING: Large deviation from Benford's Law")
        check3_status = "WARN"
    else:
        print("\n‚úÖ GOOD: Roughly follows Benford's Law distribution")
        check3_status = "PASS"

except Exception as e:
    print(f"‚ö†Ô∏è  Could not run Benford test: {e}")
    check3_status = "SKIP"

print()

# ============================================================================
# CHECK 4: Temporal Patterns
# ============================================================================
print("CHECK 4: Temporal Patterns")
print("-" * 70)

try:
    # Load verification attempts with timestamps
    verification_full = pd.read_csv('data/verification_attempts.csv')
    verification_full['timestamp'] = pd.to_datetime(verification_full['attempt_timestamp'])

    # Daily conversion rate
    verification_full['date'] = verification_full['timestamp'].dt.date
    verification_full['converted'] = (verification_full['completion_status'] == 'completed').astype(int)

    daily_stats = verification_full.groupby('date')['converted'].agg(['count', 'mean'])

    print(f"Daily conversion rate variance:")
    print(f"  Mean: {daily_stats['mean'].mean():.4f}")
    print(f"  Std Dev: {daily_stats['mean'].std():.4f}")

    # Check for weekend effects
    verification_full['day_of_week'] = pd.to_datetime(verification_full['timestamp']).dt.dayofweek
    weekend_rate = verification_full[verification_full['day_of_week'].isin([5, 6])]['converted'].mean()
    weekday_rate = verification_full[~verification_full['day_of_week'].isin([5, 6])]['converted'].mean()

    print(f"\n  Weekday conversion: {weekday_rate:.4f}")
    print(f"  Weekend conversion: {weekend_rate:.4f}")
    print(f"  Difference: {abs(weekend_rate - weekday_rate):.4f}")

    # Red flags
    if daily_stats['mean'].std() < 0.001:
        print("\nüö® RED FLAG: Perfectly smooth conversion rates (no daily variance)")
        check4_status = "FAIL"
    elif abs(weekend_rate - weekday_rate) < 0.001:
        print("\nüö® RED FLAG: No weekend effect detected")
        check4_status = "FAIL"
    else:
        print("\n‚úÖ GOOD: Natural day-to-day fluctuation present")
        check4_status = "PASS"

except Exception as e:
    print(f"‚ö†Ô∏è  Could not analyze temporal patterns: {e}")
    check4_status = "SKIP"

print()

# ============================================================================
# CHECK 5: User Segment Differences
# ============================================================================
print("CHECK 5: User Segment Differences")
print("-" * 70)

# Check device type differences
if 'device_type' in df.columns:
    segment_analysis = df.groupby(['device_type', 'variant'])['converted'].mean().unstack()

    print("Conversion rates by device type:")
    print(segment_analysis)

    # Check if mobile vs desktop are different
    if len(segment_analysis) > 1:
        device_types = segment_analysis.index.tolist()
        if len(device_types) >= 2:
            rate1 = segment_analysis.iloc[0].mean()
            rate2 = segment_analysis.iloc[1].mean()

            if abs(rate1 - rate2) < 0.001:
                print("\nüö® RED FLAG: Device types have identical conversion rates")
                check5_status = "FAIL"
            else:
                ratio = min(rate1, rate2) / max(rate1, rate2)
                print(f"\n  Device conversion ratio: {ratio:.2f}")
                if 0.5 <= ratio <= 0.9:
                    print("‚úÖ GOOD: Realistic device differences (mobile ~50-90% of desktop)")
                    check5_status = "PASS"
                else:
                    print("‚ö†Ô∏è  WARNING: Device differences seem unusual")
                    check5_status = "WARN"
        else:
            print("‚ö†Ô∏è  Only one device type found")
            check5_status = "WARN"
    else:
        print("‚ö†Ô∏è  No device segmentation found")
        check5_status = "WARN"
else:
    print("‚ö†Ô∏è  No device_type column found")
    check5_status = "WARN"

print()

# ============================================================================
# CHECK 6: P-value Reality Check
# ============================================================================
print("CHECK 6: P-value Reality Check")
print("-" * 70)

# Run chi-squared test
contingency_table = pd.crosstab(df['variant'], df['converted'])
chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)

print(f"Primary metric p-value: {p_value:.6f}")

# Check for suspicious p-values
if abs(p_value - 0.05) < 0.001 or abs(p_value - 0.01) < 0.001:
    print("üö® RED FLAG: P-value exactly at significance threshold (0.05 or 0.01)")
    check6_status = "FAIL"
elif p_value == 0.05 or p_value == 0.01:
    print("üö® RED FLAG: P-value is exactly 0.05 or 0.01 (too perfect)")
    check6_status = "FAIL"
else:
    print("‚úÖ GOOD: P-value has natural decimal precision")
    check6_status = "PASS"

print()

# ============================================================================
# SUMMARY
# ============================================================================
print("=" * 70)
print("AUDIT SUMMARY")
print("=" * 70)

checks = {
    "1. Traffic Split": check1_status,
    "2. Effect Size": check2_status,
    "3. Benford's Law": check3_status,
    "4. Temporal Patterns": check4_status,
    "5. User Segments": check5_status,
    "6. P-value Precision": check6_status
}

for check, status in checks.items():
    emoji = "‚úÖ" if status == "PASS" else "üö®" if status == "FAIL" else "‚ö†Ô∏è" if status == "WARN" else "‚è≠Ô∏è"
    print(f"{emoji} {check}: {status}")

# Overall assessment
fail_count = sum(1 for s in checks.values() if s == "FAIL")
warn_count = sum(1 for s in checks.values() if s == "WARN")
pass_count = sum(1 for s in checks.values() if s == "PASS")

print()
print(f"Results: {pass_count} PASS, {warn_count} WARN, {fail_count} FAIL")
print()

if fail_count > 0:
    print("üö® CRITICAL: This data has obvious red flags that will hurt credibility")
    print("   Recommendation: Run fix_synthetic_data.py to address issues")
elif warn_count > 2:
    print("‚ö†Ô∏è  ATTENTION NEEDED: Several areas could be improved")
    print("   Recommendation: Consider minor adjustments")
else:
    print("‚úÖ GOOD: Data passes basic credibility checks for mid-level portfolio")
    print("   Minor improvements possible but not critical")

print("=" * 70)
